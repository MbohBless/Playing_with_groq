{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaZwbjiXAdXDxTkKc0PeY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MbohBless/Playing_with_groq/blob/main/NL2SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urLeBeJ-gJDa",
        "outputId": "376a48b8-dcba-4f5e-8522-4611c2adfcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m954.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q groq\n",
        "%pip install -q duckdb\n",
        "%pip install -q sqlparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import duckdb\n",
        "import groq\n",
        "import sqlparse\n",
        "import json\n",
        "from typing import Dict, Any, List\n",
        "import logging\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "1XOrTDlxiVx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configuting logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "zQyioxUCrCKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chat_with_groq(client:groq.Groq,\n",
        "                   prompt:str,\n",
        "                   model:str,\n",
        "                   response_format: Dict[str, str]\n",
        "                   )-> Any:\n",
        "                   \"\"\"\n",
        "                   The function here works for the prompting the model for the qroq based model with natural language to\n",
        "                   geegnrate the appropriate SQL query for the provided data and prompt. So with this output, can then be used to create proper\n",
        "                   SQL queries and aggregation\n",
        "\n",
        "                   Args:\n",
        "                    client: This represents the Groq client which will be used top perform the conversion of the natural language to SQL.\n",
        "                    prompt: Bases on the user message provided, the prompt will then be converted to SQL for future use for the data querying.\n",
        "                    model: The Model here represents the based open source model that is used to make the query on. examples include llama3-70b-8192 or llama3-8b-8192 find out more here https://console.groq.com/docs/models\n",
        "                    response_format: As model has the possibility of supporting JSON and MD outputs, this is a dictionary which specifies the the required return response format. by default this for the model is usually targetted\n",
        "                    to returning an MD data but in the case of NL2SQL, then we coule work with SQL\n",
        "\n",
        "                   Returns:\n",
        "                   the function returns some data based on the response_format response form the model's response\n",
        "                   \"\"\"\n",
        "                   completion = client.chat.completions.create(\n",
        "                       model = model,\n",
        "                       messages = [\n",
        "                           {\n",
        "                               \"role\":\"user\",\n",
        "                               \"content\":prompt\n",
        "                           }\n",
        "                       ],\n",
        "                       response_format=response_format\n",
        "                   )\n",
        "                  #  logger.info(f\"Completion: {completion}\")\n",
        "                   return completion.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "id": "e8IWqIY6idDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_duckdb_query(query:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Execute a DuckDB query and return the result as a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        query (str): The DuckDB query to execute.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The result of the query as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    original_cwd = os.getcwd()\n",
        "    os.chdir('data')\n",
        "\n",
        "    try:\n",
        "        conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
        "        query_result = conn.execute(query).fetch_df().reset_index()\n",
        "    finally:\n",
        "        os.chdir(original_cwd)\n",
        "\n",
        "    return query_result"
      ],
      "metadata": {
        "id": "fpuBc2IIui52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summarization(client:groq.Groq,\n",
        "                      use_question:str,\n",
        "                      df:pd.DataFrame,\n",
        "                      model:str)->Any:\n",
        "                      \"\"\"\n",
        "                      For this query, the user input is better summarized around the provided Dataframe. This give a better contextual structure for the user to perfom the action\n",
        "\n",
        "                      Args:\n",
        "                        client: This represents the Groq client which will be used top perform the conversion of the natural language to SQL.\n",
        "                        use_question: Bases on the user message provided, the prompt will then be converted to SQL for future use for the data querying.\n",
        "                        model: The Model here represents the based open source model that is used to make the query on. examples include llama3-70b-8192 or llama3-8b-8192 find out more here https://console.groq.com/docs/models\n",
        "                        df: this is a pandas dataframe which containe the database daat which will then be used to summarize the base query around it, making the prompt more realistic.\n",
        "\n",
        "                      Returns:\n",
        "                      the function returns some json response form the model's response\n",
        "                      \"\"\"\n",
        "                      prompt= '''\n",
        "                      A user asked the following question pertaining to local database tables:\n",
        "\n",
        "                      {user_question}\n",
        "\n",
        "                      To answer the question, a dataframe was returned:\n",
        "\n",
        "                      Dataframe:\n",
        "                      {df}\n",
        "\n",
        "                      In a few sentences, summarize the data in the table as it pertains to the original user question. Avoid qualifiers like \"based on the data\" and do not comment on the structure or metadata of the table itself\n",
        "                      '''.format(user_question = use_question, df = df)\n",
        "                      return chat_with_groq(client,prompt,model,None)\n"
      ],
      "metadata": {
        "id": "8aLgnEkH39Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data download\n",
        "def download_file(url:str, file_path:str):\n",
        "    import requests\n",
        "    logger.info(f\"Downloading {url} to {file_path}\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    logging.info(f\"Downloaded {url} to {file_path}\")"
      ],
      "metadata": {
        "id": "H-G_o8fB7hzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir_if_not_available(path):\n",
        "    logging.warning(f\"Creating directory {path}\\n\")\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    return path"
      ],
      "metadata": {
        "id": "QIRzYS4m72xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = create_dir_if_not_available(\"data\")\n",
        "file_names = [\"employees.csv\",\"purchases.csv\"]\n",
        "file_locations = [\"https://raw.githubusercontent.com/groq/groq-api-cookbook/main/replit-examples/text-to-sql-json-mode/data/employees.csv\",\"https://raw.githubusercontent.com/groq/groq-api-cookbook/main/replit-examples/text-to-sql-json-mode/data/purchases.csv\"]\n",
        "\n",
        "for file_name, file_location in zip(file_names, file_locations):\n",
        "    download_file(file_location, os.path.join(data_path, file_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqXYp7XO8Qcd",
        "outputId": "0e2d8615-ceac-4eb5-b8a1-6bcdb9fb18c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Creating directory data\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_path = create_dir_if_not_available(\"prompts\")\n",
        "prompt_file_names = [\"base_prompt.txt\"]\n",
        "prompt_file_locations = [\"https://raw.githubusercontent.com/groq/groq-api-cookbook/main/replit-examples/text-to-sql-json-mode/prompts/base_prompt.txt\"]\n",
        "\n",
        "for file_name, file_location in zip(prompt_file_names, prompt_file_locations):\n",
        "    download_file(file_location, os.path.join(data_path, file_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF8BOTRi_3b_",
        "outputId": "697f2436-8a42-406e-d4ac-9904ab529ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Creating directory prompts\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "YDF0NNvaBB4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_func(base_prompt:str,model:str = \"llama3-70b-8192\",api_key:str=userdata.get(\"GROQ_API_KEY\")):\n",
        "  client = groq.Groq(api_key=api_key)\n",
        "  print(\"Welcome to Groq Text to SQL\")\n",
        "  print(\"You can as questions about the data employee.csv and purchases.csv files\")\n",
        "\n",
        "  while True:\n",
        "    user_question = input(\"Enter your question or type 'exit' to quit: \")\n",
        "    if user_question.lower() == 'exit':\n",
        "      break\n",
        "    if user_question:\n",
        "      full_prompt = base_prompt.format(user_question=user_question)\n",
        "      response = chat_with_groq(client,full_prompt,model,{\n",
        "          \"type\":\"json_object\"\n",
        "      })\n",
        "      response = json.loads(response)\n",
        "      if \"sql\" in response:\n",
        "        print(response)\n",
        "        sql_query = response[\"sql\"]\n",
        "        results_df = execute_duckdb_query(sql_query)\n",
        "\n",
        "        fotmatted_sql_query = sqlparse.format(sql_query, reindent=True, keyword_case='upper')\n",
        "        print(f\"SQL Query: {fotmatted_sql_query}\")\n",
        "        print(results_df.to_markdown())\n",
        "        summarization = get_summarization(client,user_question,results_df,model)\n",
        "        print(summarization)\n",
        "      else:\n",
        "        print(response)\n"
      ],
      "metadata": {
        "id": "AvGsqWoLBGMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " with open('prompts/base_prompt.txt', 'r') as file:\n",
        "        base_prompt = file.read()"
      ],
      "metadata": {
        "id": "LjkZofhxKznm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_func(base_prompt=base_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qsOuqBuLB1V",
        "outputId": "933280f6-2dce-4212-e576-b58ffd68904c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Groq Text to SQL\n",
            "You can as questions about the data employee.csv and purchases.csv files\n",
            "Enter your question or type 'exit' to quit: get all employees\n",
            "{'sql': 'SELECT name, email FROM employees.csv AS employees'}\n",
            "SQL Query: SELECT name,\n",
            "       email\n",
            "FROM employees.csv AS employees\n",
            "|    |   index | name              | email                  |\n",
            "|---:|--------:|:------------------|:-----------------------|\n",
            "|  0 |       0 | Richard Hendricks | richard@piedpiper.com  |\n",
            "|  1 |       1 | Erlich Bachman    | erlich@aviato.com      |\n",
            "|  2 |       2 | Dinesh Chugtai    | dinesh@piedpiper.com   |\n",
            "|  3 |       3 | Bertram Gilfoyle  | gilfoyle@piedpiper.com |\n",
            "|  4 |       4 | Jared Dunn        | jared@piedpiper.com    |\n",
            "|  5 |       5 | Monica Hall       | monica@raviga.com      |\n",
            "|  6 |       6 | Gavin Belson      | gavin@hooli.com        |\n",
            "The data shows a list of 7 employees, including Richard Hendricks, Erlich Bachman, and Dinesh Chugtai, all of whom have email addresses associated with Pied Piper, except for Monica Hall who is associated with Raviga, and Gavin Belson who is associated with Hooli.\n",
            "Enter your question or type 'exit' to quit: exit\n"
          ]
        }
      ]
    }
  ]
}